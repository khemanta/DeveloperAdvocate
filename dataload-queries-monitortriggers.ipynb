{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Historical Data and Reference OpenScale Datamart Queries\n\nThis notebook should be run in a Watson Studio project, using Default Python 3.6 runtime environment. It requires a Cloud API key to access the following Cloud services:\n* Watson OpenScale\n* Watson Machine Learning\n\nThe notebook assumes the model has been deployed to Watson Machine Learning and Watson OpenScale has been configured with various monitors. It will load historical data into OpenScale to simulate a model that has been in production for some time."
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "# 1. Setup\n\n#### 1.1 Dependencies"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install --upgrade ibm-ai-openscale --no-cache | tail -n 1\n!pip install --upgrade watson-machine-learning-client | tail -n 1",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 1.2 Configure Service Credentials\n\nUpdate the two cells below with your Cloud API Key and your Watson Machine Learning service credentials."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "CLOUD_API_KEY = \"PASTE HERE\"",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "WML_CREDENTIALS = {\n    \"apikey\": \"key\",\n    \"iam_apikey_description\": \"description\",\n    \"iam_apikey_name\": \"auto-generated-apikey\",\n    \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n    \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::\",\n    \"instance_id\": \"instance_id\",\n    \"password\": \"password\",\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"username\": \"username\"\n}",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 1.3 Model Parameters\n\n__Ensure that the two parameters match the model / deployment you have previously subscribed__"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "MODEL_NAME = \"Spark German Risk Model\"\nDEPLOYMENT_NAME = \"Spark German Risk Deployment\"",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 1.4 Gather Model Information"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import time\nfrom watson_machine_learning_client import WatsonMachineLearningAPIClient\n\nwml_client = WatsonMachineLearningAPIClient(WML_CREDENTIALS)\nwml_client.repository.list_models()\n\nmodel_uid = None\nwml_models = wml_client.repository.get_details()\nfor model_in in wml_models['models']['resources']:\n    if MODEL_NAME == model_in['entity']['name']:\n        model_uid = model_in['metadata']['guid']\n        break\n\ndeployment_uid = None\ndeployment = None\nscoring_url = None\nwml_deployments = wml_client.deployments.get_details()\nfor deployment_in in wml_deployments['resources']:\n    if DEPLOYMENT_NAME == deployment_in['entity']['name']:\n        deployment_uid = deployment_in['metadata']['guid']\n        scoring_url = deployment_in['entity']['scoring_url']\n        deployment = deployment_in\n        break\n\nif model_uid is None:\n    print(\"No model ...\")\n    \nif deployment_uid is None:\n    print(\"No Model deployment...\")\n    \nprint(\"Model id: {}\".format(model_uid))\nprint(\"Deployment id: {}\".format(deployment_uid))\nprint(\"Scoring URL: {}\".format(scoring_url))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 1.5 Get Watson OpenScale GUID\n\nEach instance of OpenScale has a unique ID. We can get this value using the Cloud API key specified at the beginning of the notebook."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from ibm_ai_openscale.utils import get_instance_guid\nfrom ibm_ai_openscale import APIClient\n\nwos_client = None\nWOS_GUID = get_instance_guid(api_key=CLOUD_API_KEY)\nWOS_CREDENTIALS = {\n    \"instance_guid\": WOS_GUID,\n    \"apikey\": CLOUD_API_KEY,\n    \"url\": \"https://api.aiopenscale.cloud.ibm.com\"\n}\n\nif WOS_GUID is None:\n    print('Watson OpenScale GUID NOT FOUND')\nelse:\n    print(\"Watson OpenScale GUID: {}\".format(WOS_GUID))\n\nwos_client = APIClient(aios_credentials=WOS_CREDENTIALS)\nprint(\"Watson OpenScale Python Client Version: {}\".format(wos_client.version))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 1.6 Get deployment subscription\n\nWe have previously subscribed Watson OpenScale to our machine learning model. Here we get that subscription."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "wos_client.data_mart.subscriptions.list()\n\nsubscriptions_uids = wos_client.data_mart.subscriptions.get_uids()\nsubscription_id = None\nfor sub in subscriptions_uids:\n    if wos_client.data_mart.subscriptions.get_details(sub)['entity']['asset']['name'] == MODEL_NAME:\n        subscription = wos_client.data_mart.subscriptions.get(sub)\n        subscription_id = sub\n        break\n            \nif subscription is None:\n    print('Subscription not found.')\n    \ndata_mart_id = subscription.get_details()['metadata']['url'].split('/service_bindings')[0].split('marts/')[1]\nprint(\"Data Mart ID: {}\".format(data_mart_id))\n\nbusiness_application_url = \"/\".join((WOS_CREDENTIALS['url'], data_mart_id,\"v2\", \"business_applications\" ))\nprint(\"Business Application URL: {}\".format(business_application_url))\n\nperformance_metrics_url = WOS_CREDENTIALS['url'] + subscription.get_details()['metadata']['url'].split('/service_bindings')[0] + '/metrics'\nprint(\"Performance Metrics URL: {}\".format(performance_metrics_url))\n\nmeasurements_url = WOS_CREDENTIALS['url'] + subscription.get_details()['metadata']['url'].split('/service_bindings')[0] + '/measurements'\nprint(\"Measurements URL: {}\".format(measurements_url))\n\nmanual_labeling_url = WOS_CREDENTIALS['url'] + subscription.get_details()['metadata']['url'].split('/service_bindings')[0] + '/manual_labelings'\nprint(\"Manual Labeling URL: {}\".format(manual_labeling_url))\n\nmonitor_instances_url = \"/\".join((WOS_CREDENTIALS['url'], data_mart_id,\"v2\", \"monitor_instances\" ))\nprint(\"Monitor Instances URL: {}\".format(monitor_instances_url))\n      \nbinding_uid = subscription.get_details()['entity']['service_binding_id']\nprint(\"Binding ID: {}\".format(binding_uid))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "***\n\n# 2. Historical Data\n\nThe next section of the notebook downloads and writes historical data to the payload and measurement tables to simulate a production model that has been monitored and receiving regular traffic for the last seven days. This historical data can be viewed in the Watson OpenScale user interface. The code uses the Python and REST APIs to write this data."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import os\nfrom IPython.utils import io\nfrom ibm_ai_openscale.utils.inject_demo_data import DemoData\n\nhistoricalData = DemoData(aios_credentials=WOS_CREDENTIALS)\nhistorical_data_path=os.getcwd()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 2.1 Insert historical payloads"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "with io.capture_output() as captured:\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/history_payloads_0.json\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/history_payloads_1.json\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/history_payloads_2.json\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/history_payloads_3.json\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/history_payloads_4.json\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/history_payloads_5.json\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/history_payloads_6.json\n!ls -lh history_payloads_*.json\n\nhistoryDays = 7\n\nprint('Starting payload loading')\nhistoricalData.load_historical_scoring_payload(subscription, deployment_uid,file_path=historical_data_path, day_template=\"history_payloads_{}.json\" )\nprint('Finished')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 2.2 Insert historical fairness metrics"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import json\nimport datetime\nimport requests\n\nwith io.capture_output() as captured:\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/history_fairness.json -O history_fairness.json\n!ls -lh history_fairness.json\n\nwith open('history_fairness.json', 'r') as history_file:\n    payloads = json.load(history_file)\n\nfor day in range(historyDays):\n    print('Loading day', day + 1)\n    metrics = []\n    \n    for hour in range(24):\n        score_time = (datetime.datetime.utcnow() + datetime.timedelta(hours=(-(24*day + hour + 1)))).strftime('%Y-%m-%dT%H:%M:%SZ')\n        index = (day * 24 + hour) % len(payloads) # wrap around and reuse values if needed\n        \n        metric = {\n            'metric_type': 'fairness',\n            'binding_id': binding_uid,\n            'timestamp': score_time,\n            'subscription_id': model_uid,\n            'asset_revision': model_uid,\n            'deployment_id': deployment_uid,\n            'value': payloads[index]\n        }\n        metrics.append(metric)\n    response = requests.post(performance_metrics_url, json=metrics, headers=wos_client._get_headers())\nprint('Finished')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 2.3 Insert historical debias metrics"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "with io.capture_output() as captured:\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/history_debias.json -O history_debias.json\n!ls -lh history_debias.json\n\nwith open('history_debias.json', 'r') as history_file:\n    payloads = json.load(history_file)\n\nfor day in range(historyDays):\n    print('Loading day', day + 1)\n    debias_metrics = []\n    for hour in range(24):\n        score_time = (datetime.datetime.utcnow() + datetime.timedelta(hours=(-(24*day + hour + 1)))).strftime('%Y-%m-%dT%H:%M:%SZ')\n        index = (day * 24 + hour) % len(payloads) # wrap around and reuse values if needed\n\n        debiasMetric = {\n            'metric_type': 'debiased_fairness',\n            'binding_id': binding_uid,\n            'timestamp': score_time,\n            'subscription_id': model_uid,\n            'asset_revision': model_uid,\n            'deployment_id': deployment_uid,\n            'value': payloads[index]\n        }\n\n        debias_metrics.append(debiasMetric)\n    response = requests.post(performance_metrics_url, json=debias_metrics, headers=wos_client._get_headers())\nprint('Finished')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 2.4 Insert historical quality metrics"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "measurements = [0.76, 0.78, 0.68, 0.72, 0.73, 0.77, 0.80]\nfor day in range(historyDays):\n    quality_metrics = []\n    print('Day', day + 1)\n    for hour in range(24):\n        score_time = (datetime.datetime.utcnow() + datetime.timedelta(hours=(-(24*day + hour + 1)))).strftime('%Y-%m-%dT%H:%M:%SZ')\n        qualityMetric = {\n            'metric_type': 'quality',\n            'binding_id': binding_uid,\n            'timestamp': score_time,\n            'subscription_id': model_uid,\n            'asset_revision': model_uid,\n            'deployment_id': deployment_uid,\n            'value': {\n                'quality': measurements[day],\n                'threshold': 0.7,\n                'metrics': [\n                    {\n                        'name': 'auroc',\n                        'value': measurements[day],\n                        'threshold': 0.7\n                    }\n                ]\n            }\n        }\n        \n        quality_metrics.append(qualityMetric)\n    \n    response = requests.post(performance_metrics_url, json=quality_metrics, headers=wos_client._get_headers())\n\nprint('Finished')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 2.5 Insert historical confusion matrixes"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "with io.capture_output() as captured:\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/history_quality_metrics.json -O history_quality_metrics.json\n!ls -lh history_quality_metrics.json\n\nwith open('history_quality_metrics.json') as json_file:\n    records = json.load(json_file)\n    \nfor day in range(historyDays):\n    index = 0\n    measurments = []\n    print('Day', day + 1)\n    \n    for hour in range(24):\n        score_time = (datetime.datetime.utcnow() + datetime.timedelta(hours=(-(24*day + hour + 1)))).strftime('%Y-%m-%dT%H:%M:%SZ')\n\n        measurement = {\n            \"monitor_definition_id\": 'quality',\n            \"binding_id\": subscription.binding_uid,\n            \"subscription_id\": subscription.uid,\n            \"asset_id\": subscription.source_uid,\n            'metrics': [records[index]['metrics']],\n            'sources': [records[index]['sources']],\n            'timestamp': score_time\n        }\n\n        measurments.append(measurement)\n        index+=1\n\n    response = requests.post(measurements_url, json=measurments, headers=wos_client._get_headers())\n\nprint('Finished')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 2.6 Insert historical performance metrics"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import random\n\nfor day in range(historyDays):\n    performance_metrics = []\n    print('Day', day + 1)\n    for hour in range(24):\n        score_time = (datetime.datetime.utcnow() + datetime.timedelta(hours=(-(24*day + hour + 1)))).strftime('%Y-%m-%dT%H:%M:%SZ')\n        score_count = random.randint(60, 600)\n        score_resp = random.uniform(60, 300)\n\n        performanceMetric = {\n            'metric_type': 'performance',\n            'binding_id': binding_uid,\n            'timestamp': score_time,\n            'subscription_id': model_uid,\n            'asset_revision': model_uid,\n            'deployment_id': deployment_uid,\n            'value': {\n                'response_time': score_resp,\n                'records': score_count\n            }\n        }\n        performance_metrics.append(performanceMetric)\n\n    response = requests.post(performance_metrics_url, json=performance_metrics, headers=wos_client._get_headers())\n\nprint('Finished')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### 2.7 Insert historical manual labeling"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "with io.capture_output() as captured:\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/history_manual_labeling.json -O history_manual_labeling.json\n!ls -lh history_manual_labeling.json\n\nwith open('history_manual_labeling.json', 'r') as history_file:\n    records = json.load(history_file)\n\nfor day in range(historyDays):\n    print('Loading day', day + 1)\n    record_json = []\n    for hour in range(24):\n        for record in records:\n            if record['fastpath_history_day'] == day and record['fastpath_history_hour'] == hour:\n                record['binding_id'] = binding_uid\n                record['subscription_id'] = model_uid\n                record['asset_revision'] = model_uid\n                record['deployment_id'] = deployment_uid\n                record['scoring_timestamp'] = (datetime.datetime.utcnow() + datetime.timedelta(hours=(-(24*day + hour + 1)))).strftime('%Y-%m-%dT%H:%M:%SZ')\n                record_json.append(record)\n    response = requests.post(manual_labeling_url, json=record_json, headers=wos_client._get_headers())\n\nprint('Finished')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "***\n\n# 3. OpenScale Monitor Triggers\n\nThese cells allow you to start the evaluation of different monitors."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "run_details = subscription.fairness_monitoring.run(background_mode=False)\ntime.sleep(10)\nsubscription.fairness_monitoring.show_table()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "drift_run_details = subscription.drift_monitoring.run(background_mode=False)\ntime.sleep(10)\nsubscription.drift_monitoring.get_table_content()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "run_details = subscription.quality_monitoring.run(background_mode=False)\ntime.sleep(10)\nsubscription.quality_monitoring.show_table()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "***\n\n# 4. Datamart Queries\n\nVarious queries against the subscription or monitor tables."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "wos_client.data_mart.get_deployment_metrics()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "wos_client.data_mart.bindings.list_assets()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "wos_client.data_mart.subscriptions.list()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "subscription.quality_monitoring.show_table()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#subscription.feedback_logging.show_table()\nsubscription.feedback_logging.print_table_schema()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "subscription.quality_monitoring.show_table()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "subscription.fairness_monitoring.show_table()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "subscription.drift_monitoring.show_table()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "subscription.payload_logging.show_table()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "subscription.payload_logging.print_table_schema()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Next steps\n\n__Return to the workshop instruction book.__\n\n\n## Credits\n\nThis notebook was adapted from the following sources:\n\n* [Monitor Models Code Pattern](https://github.com/IBM/monitor-wml-model-with-watson-openscale)\n* [OpenScale Labs](https://github.com/pmservice/OpenScale-Labs)\n* [OpenScale Tutorials](https://github.com/pmservice/ai-openscale-tutorials)\n\n#### Original Authors\n* Eric Martens, is a technical specialist having expertise in analysis and description of business processes, and their translation into functional and non-functional IT requirements. He acts as the interpreter between the worlds of IT and business.\n* Lukasz Cmielowski, PhD, is an Automation Architect and Data Scientist at IBM with a track record of developing enterprise-level applications that substantially increases clients' ability to turn data into actionable knowledge.\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}